{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Name Here:\n",
    "### Your TF Name:\n",
    "### Graduate or Undergraduate Credit: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is e29 PSET2\n",
    "##### 20180215_e29_pset2\n",
    "##### Harvard University DCE CSCI-E29\n",
    "##### Instructor: Nenad Svrzikapa\n",
    "##### Staff: Joe Palin, Kaleigh Douglas, Lena Hajjar, Phil Lodine, Alan Xie\n",
    "### Instructions:\n",
    "\n",
    "PSET2 is designed to advance your data exploration and manipulation skills with the Pandas library.  Before you get started please review and understand our Academic Integrity policy.  We understand that for some of your work you may have to use external sorces.  Please cite and be a good acedemic citizen. Remember that All homework assignments, quizzes and exams must be your independent work!\n",
    "\n",
    "Tips from staff:  \n",
    "\n",
    "1. Don't wait until the last day, our Psets require time and learning\n",
    "2. Piazza is there for you, we are a team and our goal is not to test you, we want to teach you! Please ask questions!\n",
    "3. Program a bit every day.  You can't learn these concepts by reading 2 chapters and attempting to solve the problems. Read a little, try the code, change the code,break the code, understand, learn.\n",
    "\n",
    "##### Note: All Datasets required for this pset are available on Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: \n",
    "### Created by: Lena\n",
    "### Undergraduate 2.5 points Graduate 2 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Pandas at Wal-Mart*\n",
    "\n",
    "We're going to explore some features of Pandas using at this dataset of Wal-Mart stores that opened since 1962. \n",
    "\n",
    "#### *You can't find actual pandas at Wal-Mart.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the dataset\n",
    "You can find the data in the file walmart.csv (source: https://github.com/plotly/datasets) The file has over 2000 records of Wal-Marts and Supercenters that have opened in the US since 1962. The dataset has the following columns:\n",
    "<ul>\n",
    "<li><b>storenum:</b> The store number</li>\n",
    "<li><b>OPENDATE:</b> The opening date of the store in MM/DD/YYYY format</li>\n",
    "<li><b>date_super:</b> The date that the store was converted into a supercenter in MM/DD/YYYY format. There may be empty values if the store was not converted into a supercenter</li>\n",
    "<li><b>conversion:</b> The number of conversions from Wal-Mart to Supercenter that the store has gone through. </li>\n",
    "<li><b>STREETADDR:</b> The store's street address</li>\n",
    "<li><b>STRCITY:</b> The store's city</li>\n",
    "<li><b>STRSTATE:</b> The store's state</li>\n",
    "<li><b>ZIPCODE:</b> The store's zipcode</li>\n",
    "<li><b>type_store:</b> The type of store. Possible values are Wal-mart and SuperCenter</li>\n",
    "<li><b>LAT:</b> The store's latitude</li>\n",
    "<li><b>LON:</b> The store's longitude</li>\n",
    "<li><b>MONTH:</b> The month the store opened</li>\n",
    "<li><b>DAY:</b> The day the store opened</li>\n",
    "<li><b>YEAR:</b> The year the store opened</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Getting the data\n",
    "You should have downloaded walmart.csv from the assignment. Now we need to load it into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "### Part 2: Exploring the dataimport pandas as pd\n",
    "\n",
    "# load walmart.csv into a dataframe. Hint: use the read_csv function\n",
    "# walmart = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what this data looks like. Display the first 10 rows of this dataset using Data slicing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# display first 10 rows of the walmart dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a lot of information about a pandas dataframe. In the cell below, find the number of rows and columns that are in this dataset. <br><b>HINT:</b> Remember that NumPy is the foundation of Pandas, how would you find the number of rows and columns in a NumPy array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n",
    "#number of rows\n",
    "# walRow = \n",
    "# print(\"This dataset has\", walRow, \"rows.\")\n",
    "\n",
    "#number of columns\n",
    "# walCol = \n",
    "# print(\"This dataset has\", walCol, \"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also display one or more columns of your choice. Display the type_store column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display rows that have a certain value for a column. In the cell below, create a new dataframe called \"onlyWalmarts\" that is the subset of the walmart dataframe where type_store equals \"Wal-Mart\". Print the first 10 rows of this new dataframe. How many stores have the type \"Wal-Mart\"?<br>\n",
    "<b>Hint:</b> To find the number of stores, find the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# find stores with type_store \"Wal-Mart\"\n",
    "# onlyWalmarts =\n",
    "\n",
    "# display the first 10 rows of onlyWalmart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many stores have the type \"Wal-Mart\"?\n",
    "\n",
    "# onlyWalRow = \n",
    "# print(\"There are\", onlyWalRow, \"stores with type 'Wal-Mart'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Counting Wal-Marts with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the value_counts() function, you can find the number of occurrances of different values in a dataframe. In the cell below, find the number stores with types Wal-Mart and Supercenter. Cast the result into a dataframe called typeCount.\n",
    "<br>\n",
    "<b>Hint:</b> Use the pd.DataFrame() to cast the result to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# typeCount = \n",
    "# typeCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the sum of the counts above. Is the sum the same as the number of rows that you found earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the Sum. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer this question:\n",
    "Is it the answer you got the same as the number of rows of the walmart set? : YES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the value_counts() function again, find out how many stores were converted 1 or 0 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# conversionCount = \n",
    "# conversionCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the sum of the counts above. Is the sum the same number of rows you found earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "#find the sum\n",
    "# conversionSum = \n",
    "\n",
    "# print(\"There are\", conversionSum, \"rows in the conversionCount dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer this question:\n",
    "Is it the answer you got the same as the number of rows of the walmart set? : NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely at the walmart data, you'll see that some of rows have NaN values for \"conversion.\" Those rows weren't counted in the conversionCount data above. In the next cell, find the number of stores that don't have any \"conversion\" data.\n",
    "<br>\n",
    "<b>Hint:</b> Use the .isnull( ) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# find all all the rows with NaN value for conversion\n",
    "# noConversionData = \n",
    "# find the number of stores with a NaN value for conversion\n",
    "# noConversion = \n",
    "# print(\"There are\", noConversion, \"stores with no conversion data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the number that you found from the conversion counts and the stores with no conversion data. Does it now equal the total number of rows of the walmart set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer this question:\n",
    "Is it the answer you got the same as the number of rows of the walmart set? : YES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: What does it all mean?\n",
    "A question I hope that you've been asking yourself (if you haven't already figured it out) is WHY are there stores with NaN values for \"conversion.\" Hasn't each store either been converted 1 or 0 times? What does this mean? For the next part, I using the process that you've been using for the first 3 parts, find out what types of stores have values of 1, 0, and NaN. <i><u>Are they stores that got converted to Supercenters? Are they stores that were built as Supercenters? Are they stores that were just never converted into Supercenters? </i></u>You can explore the data however you'd like, but I'd suggest you make use of the functions that you've been using and that you look at the data. \n",
    "\n",
    "<b>Hint:</b> Make 3 different data frames for stores with conversion values of 1, 0, or NaN. You already made the dataframe for the stores with NaN values. I would personally take a look at the OPENDATE, date_super, and type_store fields. If you forget what those fields mean, look at my key at the top of this page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stores with NaN for conversion\n",
    "noConversionData[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can put more code here\n",
    "\n",
    "# explore the data however you'd like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here \n",
    "\n",
    "# make a data frame of stores with 1 as a conversion value\n",
    "# convertedStores = \n",
    "\n",
    "# convertedStores[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can put more code here\n",
    "\n",
    "# explore the data however you'd like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here \n",
    "\n",
    "# make a data frame of stores with 0 as a conversion value\n",
    "# notConvertedStores = \n",
    "\n",
    "# notConvertedStores[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can put more code here\n",
    "\n",
    "# explore the data however you'd like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the following questions:\n",
    "\n",
    "1. <b>Decribe the stores with 1 as a conversion value:</b> \n",
    "\n",
    "2. <b>Describe the stores with 0 as a conversion value:</b> \n",
    "\n",
    "3. <b>Describe the stores with NaN as a conversion value:</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: \n",
    "### Created by: Kaleigh\n",
    "### Undergraduate 2.5 points Graduate 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "## Winter Olympic Games\n",
    "By: Kaleigh  \n",
    "\n",
    "Source: https://www.kaggle.com/the-guardian/olympic-games/data  \n",
    "\n",
    "The dataset contains information on winter olympic games from 1924 to 2014.  Note: The dataset uses country codes rather than the complete country names.  If you would like to learn more about the dataset or know which codes correspond to which countries, you can find that information in the \"dictionary.csv\" file located at the link above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 1) Load Dataset  \n",
    "\n",
    "- Load the \"winter.csv\" file as a pandas dataframe.  \n",
    "- Display the **first 5** rows of the dataframe.  \n",
    "- Display the **last 5** rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2) Check for Null Values\n",
    "- If you find null values in the dataset, print 'True'\n",
    "- Otherwise, print 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 3) Data Summary  \n",
    "- Use describe() to display a summary table of your data.  \n",
    "- Use either the 'include' parameter or 'exclude' parameter so that the table only includes the categorical columns (meaning it does not include the 'Year' column, which pandas interprets as numerical rather than categorical and is not particularly meaningful for our purposes).  I recommend reading the pandas documentation for describe()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 4) Bar Chart - Number of Medals Per Country\n",
    "Use the plotly library offline or cufflinks offline to create a bar chart displaying the number of medals per country.  (Country code on the x axis and number of medals on the y axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) United States Olympic Medals  \n",
    "- Create a dataframe containing only USA Olympic Medals\n",
    "- Display the **first 10** rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) In which year did the USA win the most gold medals in the Bobsleigh competition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: \n",
    "### Created by: Phil\n",
    "### Undergraduate 2.5 points Graduate 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## West Nile Virus Cases\n",
    "By: Phil  \n",
    "\n",
    "Source: https://www.healthdata.gov/dataset/west-nile-virus-cases-2006-present-1  \n",
    "\n",
    "The state of California tracks the incidence of West Nile Virus on a monthly basis, by county. This dataset contains information on the number of cases each month since 2006. You can find more information about this dataset, and the disease, at the link above.\n",
    "\n",
    "Note that there is more than one way to do the exercises in this problem!\n",
    "\n",
    "We'll start with the usual boilerplate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('wnvhumancases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset  \n",
    "\n",
    "Load the **wnvhumancases.csv** file into a pandas dataframe. How many total rows are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first 10 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some simple statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many total positive cases of West Nile have been reported over the life of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To within 2 decimal places, how many cases would the average county expect to report in any month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### West Nile Virus over time\n",
    "\n",
    "Add a plot that shows the total number of cases per year, with the year on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the graph which year had the highest total cases -- and also, that after a steep upward trajectory in the early years of the decade, the present trend appears to be down.\n",
    "\n",
    "What about the trend during the average year? Add a plot that shows the total number of cases of West Nile over the course of the year, combining the counts for all years of the dataset. (Hint: the values on the x-axis should be weeks, and your graph should be roughly bell-shaped with a peak between 350 and 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note which month appears to be the peak for cases of West Nile -- this most likely agrees with what you recall hearing on typical news reports throughout the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: \n",
    "### Created by: Joe\n",
    "### Undergraduate 2.5 points Graduate 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apples\n",
    "\n",
    "A sweet and delicious fruit.  But also, a computer ubiquitous to our class environment?\n",
    "\n",
    "Today, we'll explore historical apple stock data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the library du jour is pandas\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first.  We need to read in our dataset.  Read in the dataset \"AAPL.csv\" and store it to thet variable, df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the supplied dataset, from yahoo finance, read in the csv file\n",
    "\n",
    "# Your code here:\n",
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've read in the data, give us a quick view of the first 6 rows of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something really important in our analysis, what are the data types.  Are our numbers actually numbers?  Use a pertinent method to list out what type of data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is problematic.  We need our data to be numeric if we're going to do computations on it.  Pandas will try to coerce data to the right type, but maybe we just need to help it out.  The following loop attempts to corce our numeric data to some sort of number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in df.columns[1:]:\n",
    "    df[name] = pd.to_numeric(df[name],errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, we've converted everything that is a number into a proper float/number.  Are our datatypes now numbers of some sort?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recheck our datatypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No.  Let's take a look at our first 10 closes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the head command to find the first 5 closing prices\n",
    "df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save you a little time, there is at least one instance where the data in the rows is non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# August of 1981.  What was your deal, Apple?\n",
    "\n",
    "# Use the .loc command to see the 165th entry in the data-frame.\n",
    "df.loc[165]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we need to get rid of the 165th data entry.  Use the .drop( ) command to get rid of our bad data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(165)\n",
    "\n",
    "# alternatively, if you want to modify your dataframe in place, \n",
    "# you can include that as a parameter.\n",
    "\n",
    "# df.drop(165, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check our data frame.  Is the bad data still there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[164:166]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope!\n",
    "\n",
    "Lets recheck our data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try coercing our data to numbers again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in df.columns[1:]:\n",
    "    df[name] = pd.to_numeric(df[name],errors=\"ignore\")\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, it looks like the one row we removed was the only bad data point.  We have now successfully coerced our data to numeric form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How else could we have done this?  It helped that there was only one bad row of data, but what if there had been lots of rows of bad data?  Repeat the for loop above, but change the errors parameter so that it returns a NaN value for invalid inputs, and then use the dropna command with the inplace parameter to drop the bad rows in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given the supplied dataset, from yahoo finance, read in the csv file\n",
    "df = pd.read_csv(\"AAPL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another alternative, if we had known ahead of time that only row 165 was bad, we could have avoided the above.  Pandas will try to be your friend when you read in data.  You can tell it which rows to skip.  You can warn it there will be headers (or not), you can try to coerce dates to datetime objects.  Lots of useful features to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given the supplied dataset, from yahoo finance, read in the csv file\n",
    "df = pd.read_csv(\"AAPL.csv\",skiprows=[166])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're interested in the closing data and corresponding dates.  Let's\n",
    "# create a new data frame with just the dates and closing prices\n",
    "closes = df[[\"Date\",\"Close\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have times listed as strings.  We have two options on how to deal with this.  We can segment the strings we're given and turn them into years, months, and days.  Or, we can change the strings into time objects.  This week we'll try the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closes[[\"Date\",\"Close\"]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_to_ordinal(date_string):\n",
    "    # we know that our dates are of the form yyyy-mm-dd\n",
    "    # we'll leverage this to turn these dates in to approximating numbers\n",
    "    \n",
    "    data = [int(x) for x in date_string.split(\"-\")]\n",
    "\n",
    "    ordinal = data[0] + (data[1]-1) / 12 + (data[2]-1) / 31 /12\n",
    "    \n",
    "    return ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our str_to_ordinal function in hand, we need to convert all of the strings to ordinals.  Use the .applymap( ) method to changes your series of Date strings to a series of Date numbers, and use the .loc( ) method to store this series over the existing strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "closes.loc[:,\"Date\"] ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to know which dates occured within the last year of a particular date, we can do a subtraction and see what is between 0 and 1.\n",
    "\n",
    "Create a Pandas series which is the difference between our calculated dates, and the string to ordinal conversion of October 10th of 1993."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "timediff = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the time differences to find every time which was in the year immediately preceding our date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "mask = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take our mask and find the dates which were within one year of our given date.  Use the numpy mean command to find the average of the values in the year preceding our date.  Also use numpy to find the 25th and 75th percentile of the data from the previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing we'll do this week.  We're going to put together all the things we've bene working on.  Create three new columns for our data frame.  One new column for the mean of the data we have in the previous year, the 25th percentile of the data from the preceding year, and the 75th percentile from the preceding year.\n",
    "\n",
    "To do this, you'll need to write a for loop which iterates over each date you have, creates a mask to extract the closing values for the dates in the year preceding each iterated date, and takes all the means of preceding year data, and turns that into a new column for your data frame.  You should do this as well for the 1st and 3rd quartiles you also calculated above.\n",
    "\n",
    "I'd suggest creating the data you want to add to the dataframe, and then merging the data in a separate step.  For more on merging, see section notes/recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Your code for taking means and quartiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code for merging your computations with the existing data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what does your final data frame look like?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: \n",
    "### Created by: Alan\n",
    "### Undergraduate 2.5 points Graduate 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go to the movies!\n",
    "\n",
    "Throughout the course of this problem, we're going to use the `pandas` library to explore a film dataset. Don't worry — we won't have to scrape any data — but we will be exploring the `pandas.DataFrame` data structure. The dataset provided contains all of the major movies that were released theatrically between 2005 and 2010 in the United States.\n",
    "\n",
    "**NOTE:** Executing given cells in this problem more than once _may_ break the problem. If you encounter errors (that are not supposed to be there) please restart your kernel and try again. If this doesn't solve the problem, let us know on Piazza!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure that we've imported `pandas`. The convention is to **`import pandas as pd`** so as to have an abbreviation for the library name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import our data! It turns out that `pandas` has a really handy method called `read_csv()` which allows us to import any CSV file into a DataFrame. Note that `pandas` automatically assumes that the first row of our CSV is our table headers. However, we have to specify with the `index_col` parameter that the zero-th column is our index — otherwise, it will get read in as an extra column of data and be redundant with the default index created by the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movies.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `pd.DataFrame.head()` method to look at the first five rows of our DataFrame. (Note that we can use the corresponding `tail()` method to look at the last five rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the data itself. We have a movie's title, runtime, MPAA rating, date of theatrical release, director, actors, and plot. Let's take a look at individual columns first and see what types of data we're dealing with. When importing data with `pd.read_csv()`, `pandas` usually assumes certain things about the types of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Each column is of type ', type(movies['title']))\n",
    "print('The title column is of type ', type(movies['title'][0]))\n",
    "print('The runtime column is of type ', type(movies['runtime'][0]))\n",
    "print('The theatrical release date column is of type ', type(movies['theatrical_release'][0]))\n",
    "print('The director column is of type ', type(movies['director'][0]))\n",
    "print('The actor column is of type ', type(movies['actor'][0]))\n",
    "print('The plot column is of type ', type(movies['plot'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! It looks like we mostly have columns with strings, except for our runtime column is (as expected) an integer. However, note that `pandas` has no problem importing columns with mixed types if they exist; on top of that, this process is usually slower than importing fixed-type columns. You can force a fixed-type import by specifying the `dtype` parameter to `pd.read_csv()`, which is usually a dictionary of column names and expected data types.\n",
    "\n",
    "Looking more closely at our column types, we'll see a couple of wrinkles. First, note that the theatrical release column is of type `str` and not `datetime`. Secondly, note that our director and actor columns are a single string despite representing (usually) one or more people. We'll have to deal with these issues later on in searching the data, so let's run a pre-processing step now.\n",
    "\n",
    "Let's use the `pd.DataFrame.apply()` method, which allows us to efficiently iterate through all of the rows in a column (or the full DataFrame) by applying a function to every row. We can apply a function that we've defined elsewhere, or use a simple one-line `lambda` function. Let's process the theatrical release column as an example, and also use the EXTREMELY useful `pd.to_datetime()` function with `pd.DataFrame.apply()` to cast the entire column to a `pd.Timestamp` type! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies['theatrical_release'] = movies['theatrical_release'].apply(lambda date: pd.to_datetime(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The theatrical release column is now of type ', type(movies['theatrical_release'][0]))\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think about the director and actor columns. You might be asking — isn't it bad practice to have potentially multiple individuals contained in a single string? The short answer is yes. It turns out that this data originated from a JSON export from a MongoDB (non-relational or NoSQL) database, where the director and actor columns were actually lists of strings. This nested data-type representation doesn't translate well to the relational row-column format of a `pandas` DataFrame, which is why we have the non-ideal situation where multiple individuals are contained in a single string in a single column. We could blow up the director and actor columns into multiple columns, but then we run into a couple of different problems: what happens to films with differently sized casts? Do we only store the X most important cast members? These are data model design questions that I'll leave as an exercise to the reader.\n",
    "\n",
    "In the meantime, we're going to check our dataset for `nan`. If you tuned in to this weekend's section, you'll recall that we discussed the concept of `nan`, or **Not a Number**. This pesky data type is technically a `float` but usually represents a missing data point. Rather than break the entire `pd.read_csv()` operation, `pandas` just assumed you meant to have a missing value in that cell and populated it by default with `numpy.nan` instead. (Note that the native Python `math` module has an implementation of `nan` as well.)\n",
    "\n",
    "Why don't we take a look in our DataFrame and see where (if at all) we have `nan` values? We'll use a Boolean mask to do so. Recall this topic from this weekend's section as well. As a refresher, a Boolean mask is an array of True/False values that correspond to the result of some operation — in this case the `pd.DataFrame.isnull()` method — and we can use it as a mask to restrict our DataFrame to a subset of rows that correspond to only the True values in our Boolean mask. \n",
    "\n",
    "**IMPORTANT:** In `pandas`, we can construct a Boolean mask on any column using any kind of comparison operator (<, >, ==) and then use the resulting mask as a filter on our original DataFrame to yield a subset of rows that match our restriction. This is a major way of accessing subsets of your data quickly! (Although it is not the only way and perhaps not the fastest depending on what you want to do and what your data looks like.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = movies.isnull().values\n",
    "movies[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just drop these two rows from our DataFrame. (A Google search reveals that these were documentaries - hence the lack of cast.) In some cases, we might want to use something like the `pd.DataFrame.fillna()` method to fill all of our `nan` cells with some default value, but in this one we'll just drop the rows from our DataFrame using `pd.DataFrame.dropna()`. If we were to train an eventual model to predict box office gross, we wouldn't want to necessarily keep these movies for potential selection into our training set, as they're relatively niche documentary films that are likely unrepresentative of the dataset.\n",
    "\n",
    "_DIGRESSION ON SEMANTICS_: Note that I'm referring to these methods as `pd.DataFrame.dropna()`. That's because they're defined as methods on the DataFrame class in `pandas`. These are not public methods that exist in `pandas` by themselves - they can only be called on an instance of the `pd.DataFrame` class, of which our `movies` variable is one. Contrast this to the `pd.read_csv()` method.\n",
    "\n",
    "Note that after we drop these two rows, our set of NaN values is non-existent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = movies.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[movies.isnull().values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the coding! This part will be relatively hands-off so you're free to interact with the DataFrame however you wish, so long as you use one of the following methods: Boolean masks, `pd.DataFrame.str.contains`, `pd.DataFrame.query`, `pd.DataFrame.iloc`, `pd.DataFrame.loc`, `pd.DataFrame.ix`, `pd.DataFrame.at`, `pd.DataFrame.iat`. I'll leave it an exercise to the reader to brush up on the documentation for said methods. Don't worry if you prefer to stick to one method - just be aware that your DataFrame manipulation toolkit has a number of different approaches! We will go into the various pros/cons and efficiencies of each method at a later point as we dive deeper into `pandas`.\n",
    "\n",
    "Please write your code below (making sure that it runs!) and also write your answers in a Markdown cell. May the odds be ever in your favour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) How many movies did David Fincher direct?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) How many movies are rated R?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Which movies did Robert Downey Jr. act in?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) What is the plot of the movie _Blades of Glory_ (2007)?**  \n",
    "Hint: it's located in row 633."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) What is the latest release date in this dataset? For which movie(s)?**  \n",
    "Hint 1: You can use `pd.to_datetime('YYYY-MM-DD')` to generate a Timestamp for any date.  \n",
    "Hint 2: Your answer (or part of it) will correspond to the holiday that the last CSCI E-29 lecture took place on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) What is the range of runtimes in our dataset? For which films do they correspond to?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Optional challenge: Who was the most prolific director(s)? The most prolific actor(s)?**  \n",
    "\n",
    "Hint: How efficiently can you solve this problem? Because our director and actor columns are merged strings of individuals, you'll need to use something like `pd.DataFrame.apply` to create new columns that separate the names. You may also want to rely on the `Counter` datatype or something like `itertools.chain.from_iterable` in your code. Each question is doable in one line of code.  \n",
    "\n",
    "Note: If you can't solve this problem, feel free to skip it! No points will be deducted. You might have to do something like this in the real world at some point though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
